from google.cloud import storage
from google.cloud import aiplatform
import os

# Import the specific classes for Gemini models
from vertexai.preview.generative_models import GenerativeModel, Part, GenerationConfig

# Initialize Google Cloud Storage client
storage_client = storage.Client()

# Initialize Vertex AI client
# Make sure 'My Firts Project' is replaced with your actual Google Cloud Project ID
# and 'us-central1' with your desired region where Gemini is available.
aiplatform.init(project=os.environ.get('GCP_PROJECT', 'My First Project'), location='us-central1')

# --- Global or function-level initialization for the model ---
# It's often better to initialize the model once globally if your Cloud Run service
# will handle many requests, to avoid re-initializing on every invocation.
# However, if your Cloud Run instance spins up and down frequently (scales to zero),
# initializing inside the function is also fine.
model = GenerativeModel("gemini-1.5-flash") # Or "gemini-1.5-flash" for the latest stable

# Define generation configuration (optional, but good practice)
# You can adjust temperature, top_p, top_k as needed for your desired output
config = GenerationConfig(
    temperature=0.1,    # Lower values make the output more deterministic
    top_p=0.95,         # Nucleus sampling: only consider tokens from the top p probability mass
    top_k=40,           # K sampling: only consider the top k tokens
    max_output_tokens=2048 # Adjust based on expected output length
)

def process_document(event, context):
    """
    Cloud Run service entry point triggered by a new file in GCS.
    """
    bucket_name = event['bucket']
    file_name = event['name']

    input_bucket = storage_client.bucket(bucket_name)
    input_blob = input_bucket.blob(file_name)


    output_bucket_name = os.environ.get('OUTPUT_BUCKET_NAME', 'poc-curated-data')
    output_bucket = storage_client.bucket(output_bucket_name)

    print(f"Processing file: {file_name} from bucket: {bucket_name}")

    try:
        # 1. Read the document
        document_content = input_blob.download_as_text()
        print(f"Document content read. Length: {len(document_content)} bytes.")

        # --- Injecting your prompt and document content ---
        # Define your specific instructions for Gemini
        user_instructions = (
            "You are an expert data formatter tasked with manipulating text documents."
            "Your task is to analyze the following document, extract key entities "
            "FORMAT ALL DATE FIELDS IN YYYYMMDD"
            "Create fields FirstName, LastName, MiddleName & Suffix from Name field"
            "Format Name Field as 'LAST, FIRST MIDDLE SUFFIX'"
            "DELETE 'LBS' FROM WEIGHT fields"
            "FORMAT GENDER AS MALE OR FEMALE OR OTHER"
            "Export manipulated objects as a txt file"
        )

        # Combine your instructions with the document content
        # The document content itself becomes part of the prompt
        full_prompt_for_gemini = f"{user_instructions}\n\nDocument:\n{document_content}"

        print("Invoking Gemini 1.5 Flash...")
        # 2. Invoke Vertex AI Gemini 1.5 Flash
        response = model.generate_content(
            contents=[Part.from_text(full_prompt_for_gemini)],
            generation_config=config, # Pass your defined configuration
        )

        # Access the generated text from the response
        if response.candidates:
            transformed_content = response.candidates[0].text
            print("Gemini response received.")
            # You might want to add JSON parsing and validation here
            # e.g., json_output = json.loads(transformed_content)
        else:
            print("No candidates found in Gemini response.")
            transformed_content = "Error: No content generated by Gemini."

        # --- Decide on the output file name and path ---
        # You might want to save the output with a different prefix or extension,
        # especially if it's now JSON.
        output_blob_name = f"curated/{os.path.splitext(file_name)[0]}.json"
        output_blob = output_bucket.blob(output_blob_name)

        # 4. Upload the transformed content to Bucket 2
        output_blob.upload_from_string(transformed_content)
        print(f"Curated document '{output_blob_name}' uploaded to '{output_bucket_name}'.")

    except Exception as e:
        print(f"Error processing document {file_name}: {e}")
        # Consider moving the failed file to a DLQ bucket here
        # input_blob.copy(storage_client.bucket('your-dlq-bucket').blob(f"failed/{file_name}"))
        raise # Re-raise to indicate failure, allowing Eventarc to retry